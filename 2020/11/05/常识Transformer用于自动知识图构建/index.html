

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="caoyusang">
  <meta name="keywords" content="">
  <title>常识Transformer用于自动知识图构建 - caoyusang的科研日常</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
    
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>caoyusang</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/7.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container page-header text-center fade-in-up">
            <span class="h2" id="subtitle">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2020-11-05 19:08" pubdate>
        2020年11月5日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      597 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      9
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto" id="post">
            <!-- SEO header -->
            <h1 style="display: none">常识Transformer用于自动知识图构建</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：5 个月前
                
              </p>
            
            <div class="markdown-body" id="post-body">
              <h3 id="《COMET：Commonsense-Transformers-for-Automatic-Knowledge-Graph-Construction》"><a href="#《COMET：Commonsense-Transformers-for-Automatic-Knowledge-Graph-Construction》" class="headerlink" title="《COMET：Commonsense Transformers for Automatic Knowledge Graph Construction》"></a>《COMET：Commonsense Transformers for Automatic Knowledge Graph Construction》</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.05317v2.pdf">论文地址</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/atcbosselut/comet-commonsense">论文源码</a></p>
<h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><ul>
<li><strong>目的层面</strong> —— 根据两个当前最常用的常识知识图<strong>ATOMIC</strong>和<strong>ConceptNet</strong>构建一个用于<strong>开发常识知识</strong>的自适应生成模型<strong>COMET</strong>，以协助完成常识知识的<strong>自我补充</strong>。</li>
</ul>
<p>​    <strong>COMET</strong>是一个自适应框架，用于通过在知识元组的种子集上训练语言模型来从语言模型构建常识知识库。这些知识元组为<strong>COMET</strong>提供了必须学习的知识库结构和关系，<strong>COMET</strong>学习调整从预处理中学习的语言模型表示，以向种子知识图添加新的节点和边。</p>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/1.png" srcset="/img/loading.gif" class="">
<ul>
<li><strong>实验层面</strong> <ul>
<li>训练知识库 —— 格式为 <script type="math/tex">\{s ,r, o\}</script>的自然语言元组, 其中 <script type="math/tex">s</script> 为元组的短语主语,  <script type="math/tex">r</script> 为元组的关系， <script type="math/tex">o</script> 为元组的短语宾语。例如 <script type="math/tex">\left(s="take \space a \space nap", r=Causes, o="have \space energy" \right)</script>。</li>
<li>任务 —— 给定 <script type="math/tex">s</script> 和 <script type="math/tex">r</script> 作为输入， 要求生成 <script type="math/tex">o</script>。 </li>
</ul>
</li>
</ul>
<h4 id="Transformer语言模型"><a href="#Transformer语言模型" class="headerlink" title="Transformer语言模型"></a>Transformer语言模型</h4><p>​    采用<strong>GPT</strong>的语言模型架构构建<strong>COMET</strong>。</p>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/2.png" srcset="/img/loading.gif" class="">
<h5 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h5><p>​    多头注意力机制 —— 顾名思义， 注意力由多个头部组成，每个头部使用查询 <script type="math/tex">Q</script> 和键 <script type="math/tex">K</script> 来计算一个唯一的点积注意力分布。</p>
<script type="math/tex; mode=display">
\text { ATTENTION }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>​    其中，<script type="math/tex">d_{k}</script> 为 <script type="math/tex">Q, \space K, \space V</script> 的输入向量的维度。对于每个注意力头来说，在计算注意力之前， <script type="math/tex">Q, \space K, \space V</script> 被一组矩阵唯一映射。(<strong>对应a图蓝色部分</strong>)</p>
<script type="math/tex; mode=display">
H_{i} = \text { ATTENTION }(QW_{i}^{Q}, KW_{i}^{K}, V_{i}^{V})</script><p>​    <script type="math/tex">H_{i}</script> 为单个注意力头的输出， <script type="math/tex">W_{i}^{Q}, \space W_{i}^{K}, \space W_{i}^{V}</script> 为特定头的关于 <script type="math/tex">Q, \space K, \space V</script> 的映射矩阵。</p>
<p>​    同时，多个注意力头的输出将被连接在一起。(<strong>对应a图紫色部分</strong>)</p>
<script type="math/tex; mode=display">
\operatorname{MULTIH}(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\left[H_{1} ; \ldots ; H_{b}\right] W^{O}</script><p>​    其中 <script type="math/tex">W^{O}</script> 为输出的映射矩阵。</p>
<p><strong>多头注意力的输入</strong></p>
<p>​    如b图所示，当前层多头注意力的输入来源于上一层Transformer Block的输出。其中，输入的 <script type="math/tex">Q</script> 对应于上一层Block的输出 <script type="math/tex">h_{t}^{l-1}</script>，而 <script type="math/tex">K</script> 和 <script type="math/tex">V</script> 对应于所有先前时间步长的前一层块的输出 <script type="math/tex">\mathbf{h}_{t}^{l-1} = \left\{h^{l-1}\right\}_{<t}</script> 。</p>
<script type="math/tex; mode=display">
\operatorname{MULTIATTN}\left(h_{t}^{l-1}\right)=\operatorname{MULTIH}\left(h_{t}^{l-1}, \mathbf{h}_{t}^{l-1}, \mathbf{h}_{t}^{l-1}\right)</script><h5 id="Transformer块"><a href="#Transformer块" class="headerlink" title="Transformer块"></a>Transformer块</h5><p>​    如b图所示，每个Transformer层包含一个架构上相同的Transformer Block块(尽管具有唯一的可训练参数)，该Transformer Block对该块的输入应用以下变换:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\tilde{g}^{l} &=\mathrm{MULTIATTN}\left(h^{l-1}\right) \\
g^{l} &=\mathrm{LAYERNORM}\left(\tilde{g}^{l}+h^{l-1}\right) \\
\tilde{h}^{l} &=\mathrm{FFN}\left(g^{l}\right) \\
h^{l} &=\text { LAYERNORM }\left(\tilde{h}^{l}+g^{l}\right)
\end{aligned}</script><p>​    其中，<script type="math/tex">\tilde{g}^{l}</script> 为多头注意力的输出，<script type="math/tex">FFN</script> 是一个两层的前馈神经网络(Feedforward Network), 而 <script type="math/tex">LAYERNORM</script>层是对多头注意力层输出以及前馈层输出的正则化操作，操作的输入包含一个残差连接，该连接将前一个操作的输出和输入相加。</p>
<h5 id="输入编码器"><a href="#输入编码器" class="headerlink" title="输入编码器"></a>输入编码器</h5><p>​    <strong>文本嵌入</strong> —— <script type="math/tex">\mathbf{X}= \{ X^{s}, \space X^{r}, \space X^{o} \}</script>,  其中 <script type="math/tex">X^{s}, \space X^{r}, \space X^{o}</script> 分别代表知识元组 <script type="math/tex">\{s ,r, o\}</script> 中每一项的单词序列，<script type="math/tex">\mathbf{X}</script> 为三者的串联。对于任意 <script type="math/tex">x_{t} \in \mathbf{X}</script> ，单词 <script type="math/tex">x_{t}</script> 的单词嵌入为 <script type="math/tex">e_{t} = \sum x_{t}^{i}</script> ，这里的 <script type="math/tex">\sum</script> 为向量和。</p>
<p><strong>为何单词嵌入是一种向量和的形式？</strong></p>
<p>这是因为GPT采用<strong>字节对编码</strong>(BPE)的方式构建字词，准确来说，GPT以子词表中子词作为词嵌入的基本单位的，这里输入的文本中的完整单词在字词表中可能是好几个字词之和，所以在表示完整单词的文本嵌入时，用的是子词嵌入向量的和。关于<strong>字节对编码</strong>的具体步骤，我之前的一篇<a target="_blank" rel="noopener" href="https://caoyusang.github.io/2020/11/04/%E6%9E%84%E5%BB%BA%E5%8A%A8%E6%80%81%E7%9F%A5%E8%AF%86%E8%B7%AF%E5%BE%84%E7%94%9F%E6%88%90%E5%99%A8%E7%94%A8%E4%BA%8E%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86/">文章</a>中有提到，这里不再赘述。    </p>
<p>​    <strong>位置嵌入</strong> —— 因为transformer是一种自注意力模型，没有token顺序的概念，所以引入位置编码 <script type="math/tex">p_{t}</script> 来指示模型各个token在文本序列中所处的绝对位置。</p>
<p>​    <strong>最终输入</strong> —— <script type="math/tex">h_{t}^{l} = e_{t} + p_{t}</script>， 其中 <script type="math/tex">l</script> 表示transformer层数， <script type="math/tex">t</script> 代表第 <script type="math/tex">t</script> 个时间步。如c图所示，第一层的输入为： <script type="math/tex">h^{0} = e_{0} + p_{0}</script>，</p>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul>
<li><strong>训练任务</strong> —— 给定知识元组 <script type="math/tex">\{ s, \space r, \space o \}</script>，要求以 <script type="math/tex">s</script> 和 <script type="math/tex">r</script> 对应token的串接 <script type="math/tex">\left[ X^{s}, X^{r} \right]</script> 作为输入，模型需要学会生成 <script type="math/tex">o</script> 对应的那些tokens，即 <script type="math/tex">X^{o}</script> 。</li>
<li><strong>损失函数</strong> —— 标准的语言模型的损失函数，即对数似然函数。</li>
</ul>
<script type="math/tex; mode=display">
\mathcal{L}=-\sum_{t=|s|+|r|}^{|s|+|r|+|o|} \log P\left(x_{t} \mid x_{<t}\right)</script><p>​    已知前 <script type="math/tex">t-1</script> 个生成的token <script type="math/tex">x_{<t}</script> ，当前第 <script type="math/tex">t</script> 个位置生成token <script type="math/tex">x_{t}</script>的概率为：</p>
<script type="math/tex; mode=display">
P\left(x_{t} \mid x_{<t}\right)=\operatorname{softmax}\left(\mathbf{W}_{\text {vocab}} \cdot \mathbf{h}_{\mathrm{t}}\right)</script><p>​    其中，<script type="math/tex">|s|, \space |r|, \space |o|</script> 分别为短句主语、关系、宾语对应的token数目。 这里 <script type="math/tex">h_{t}</script> 表示在解码时GPT对<script type="math/tex">x_{t}</script> 的最终表示，<script type="math/tex">\mathbf{W}_{\text {vocab}}</script>是GPT使用的词汇表的嵌入矩阵。</p>
<ul>
<li><p><strong>实验数据集</strong> —— ATOMIC 和 ConceptNet</p>
</li>
<li><p><strong>输入构造</strong></p>
<ul>
<li>ATOMIC —— 关系token只有一个</li>
</ul>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/3.png" srcset="/img/loading.gif" class="">
<ul>
<li>ConceptNet —— 关系token可能有多个，引入第二组mask来分隔关系token和宾语token。</li>
</ul>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/4.png" srcset="/img/loading.gif" class="">
</li>
<li><p><strong>模型参数初始化</strong> —— 使用GPT的预训练模型权重初始化模型，为了微调，往词汇表中加入了一些特殊的token。例如，关系嵌入(如oReact for ATOMIC和IsA for ConceptNet)是通过从标准正态分布中采样来初始化的。</p>
</li>
<li><p><strong>超参设置 </strong></p>
<ul>
<li><strong>12</strong> layers</li>
<li><strong>768</strong>-dimensional hidden states </li>
<li><strong>12</strong> attention heads</li>
<li>dropout    <strong>0.1</strong></li>
<li>activation fuction    <strong>GeLU</strong></li>
<li>batch size    <strong>64</strong></li>
<li>lr    <strong>6.25e-5</strong> (with a warmup period of 100 minibatches)</li>
<li>decay method    <strong>linear</strong></li>
</ul>
</li>
</ul>
<h4 id="实验细节"><a href="#实验细节" class="headerlink" title="实验细节"></a>实验细节</h4><h5 id="ATOMIC"><a href="#ATOMIC" class="headerlink" title="ATOMIC"></a>ATOMIC</h5><h6 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h6><ul>
<li>877K 知识元组，涵盖围绕特定事件的社会常识知识</li>
<li>九个关系维度提炼事件中的常识知识</li>
</ul>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/5.png" srcset="/img/loading.gif" class="">
<p>​    对应到实验中，ATOMIC事件(例如，“<strong>X goes to the store</strong>”)是短语主体 <script type="math/tex">s</script> ，<strong>xIntent</strong>是短语关系 <script type="math/tex">r</script> ，<strong>causes/effects</strong>(例如，“<strong>to get food</strong>”)是短语客体 <script type="math/tex">o</script>。训练集/开发集/测试集的数目分别为：<strong>710k/80k/87k</strong></p>
<h6 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h6><ul>
<li><strong>自动评估</strong></li>
</ul>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/6.png" srcset="/img/loading.gif" class="">
<p>​    上图为采用自动评估标准的评估结果(评估的是生成 <script type="math/tex">o</script> 的质量和新颖性，第一列对比的模型为<strong>ATOMIC</strong>提出文章中的<strong>baseline</strong>，后面两个是论文提出的COMET模型。从第二列开始都是评估的指标，第二列是困惑度<strong>PPL</strong>，第三列是<strong>BLEU-2</strong>，第三列是模型生成元组所占的比例，第四列是模型生成的未出现在训练集元组中 <script type="math/tex">o</script> 所占的比例(<strong>元组新颖性</strong>)，为了证明模型生成的元组新客体不是唯一的，把产生的新客体的数目作为测试集事件产生的唯一客体集合的函数，就是第五列。</p>
<ul>
<li><strong>人工评估</strong></li>
</ul>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/7.png" srcset="/img/loading.gif" class="">
<p>​    <strong>BLEU-2</strong>的评估结果表明，<strong>COMET</strong>模型超越了所有baseline的表现，比表现最好的baseline实现了51%的相对表现提升。对于人工评估的结果，<strong>COMET</strong>报告了统计上显著的<strong>相对Avg性能</strong>，比最高基线提高了18%。</p>
<p>​    为了评估在大型语料库上的预训练如何帮助模型学习产生知识，训练了一个没有用预训练权重初始化的COMET版本(COMET(- pretrain))。通过在不同比例的训练数据上训练模型来评估方法的数据效率。</p>
<p>​    最后，因为最终目标是能够执行高质量、多样化的知识库构建，所以探索了各种解码方案如何影响候选知识元组的质量，采用了不同生成策略进行了实验，这些策略包括：argmax贪婪解码、波束大小的波束搜索、b=2、5、10和k = 5、10的top-k采样。对于每种解码方法，对每种方法产生的最终候选数进行人工评估，结果如下：</p>
<img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/8.png" srcset="/img/loading.gif" class="">
<p>​    上述结果表明，使用贪婪解码来产生知识元组，与人工评估ATOMIC测试集相比，仅存在10%的相对性能差距，表明由模型产生的知识接近人工性能。</p>
<h5 id="ConceptNet"><a href="#ConceptNet" class="headerlink" title="ConceptNet"></a>ConceptNet</h5><h6 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h6><ul>
<li>标准的SRO三元组格式，涵盖大量关系三元组，例如(take a nap,  Causes,  have energy)</li>
</ul>
<p>​    对应到实验中，各选取了1200个三元组作为测试集和开发集，包含34个关系类型的100k版本的训练集用于训练模型。</p>
<h6 id="模型评估-1"><a href="#模型评估-1" class="headerlink" title="模型评估"></a>模型评估</h6><img src="/2020/11/05/%E5%B8%B8%E8%AF%86Transformer%E7%94%A8%E4%BA%8E%E8%87%AA%E5%8A%A8%E7%9F%A5%E8%AF%86%E5%9B%BE%E6%9E%84%E5%BB%BA/9.png" srcset="/img/loading.gif" class="">
<p><strong>生成质量评估</strong></p>
<p>​    为了评估生成知识的质量，给出在测试集中生成的正面示例的数量，这些正面示例被评为正确。对于给定的 <script type="math/tex">s \space r \space o \space</script>元组，考虑该模型产生元组是否正确的概率，以50%的概率对分数进行阈值化，以识别正确的预测。</p>
<p>​    结果表明，该模型可以生成高质量的知识：上表中的<strong>低困惑度</strong>(<strong>PPL</strong>)分数表明其预测的高模型置信度，而高分类器得分Score<strong>(95.25%)</strong>表明知识库补全模型在大多数情况下将生成的元组评分为正确。在人工评估(遵循与ATOMIC相同的设计)中，贪婪解码得到的元组的91.7%被评为正确。</p>
<p><strong>生成新颖度评估</strong></p>
<p>​    其中<script type="math/tex">N/T \space sro</script> 达到了<strong>59.25%</strong>，说明有接近<strong>6成</strong>的生成元组未出现在训练集中，显示该模型能够在节点之间生成新的边，甚至创建新的节点(<script type="math/tex">N/T \space o = 3.75</script> ，即<strong>3.75%的节点是新的</strong>)来扩展知识图的大小。但是需要注意的是，有一些新产生的元组仅仅是训练集中元组的简化形式。为此论文进行了研究，这些简化形式的新元组到底有多少。结论是<strong>大多数产生的新元组与训练集中它们最接近的元组具有明显不同的单词序列。</strong></p>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>​    本文引入常识转换器<strong>(COMET)</strong>来自动构建常识知识库。<strong>COMET</strong>是一个框架，用于调整语言模型的权重，以学习生成新颖和多样的常识知识元组。在两个常识知识库<strong>ATOMIC</strong>和<strong>ConceptNet上</strong>的实验结果表明，<strong>COMET</strong>经常产生人类评估者认为是正确的新常识知识。这些积极的结果表明未来可以寻求将该方法扩展到各种其他类型的知识库上，以及研究<strong>COMET</strong>是否可以学习为任意知识种子产生OpenIE风格的知识元组。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86/">常识推理</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/NLP/">NLP</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2020/11/16/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E2%80%94%E2%80%94%E5%BC%80%E5%88%9BNLP%E6%96%B0%E7%BA%AA%E5%85%83/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">预训练模型——开创NLP新纪元</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2020/11/04/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%BB%BB%E5%8A%A1%E6%A2%B3%E7%90%86/">
                        <span class="hidden-mobile">自然语言处理任务梳理</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '#post-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "常识Transformer用于自动知识图构建&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  











</body>
</html>
